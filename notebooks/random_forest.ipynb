{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m../scripts/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutl\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdecision_tree\u001b[39;00m \u001b[39mimport\u001b[39;00m Decision_Tree_Classifier\n",
      "File \u001b[1;32me:\\JupyterNotebooks\\2023\\ML Models\\MLModels\\notebooks\\../scripts\\decision_tree.py:257\u001b[0m\n\u001b[0;32m    253\u001b[0m                     entropy \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39mp\u001b[39m*\u001b[39mmath\u001b[39m.\u001b[39mlog2(p)\n\u001b[0;32m    254\u001b[0m         \u001b[39mreturn\u001b[39;00m entropy\n\u001b[1;32m--> 257\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDecision_Tree_Regressor\u001b[39;00m(Decision_Tree_Classifier):\n\u001b[0;32m    258\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, min_n_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "File \u001b[1;32me:\\JupyterNotebooks\\2023\\ML Models\\MLModels\\notebooks\\../scripts\\decision_tree.py:306\u001b[0m, in \u001b[0;36mDecision_Tree_Regressor\u001b[1;34m()\u001b[0m\n\u001b[0;32m    303\u001b[0m     yp_np \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m    304\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean((y_np \u001b[39m-\u001b[39m yp_np)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_split_pred\u001b[39m(\u001b[39mself\u001b[39m, df_s: pd\u001b[39m.\u001b[39mDataFrame, c: \u001b[39mstr\u001b[39m, i: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[pd\u001b[39m.\u001b[39mSeries, \u001b[39mfloat\u001b[39m]:\n\u001b[0;32m    307\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Splits the dataframe at the given index. Returns the predicted value as the mean of the target before and after the split.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39m        float: column value at which the split was performed (average of df_s[c].iloc[i] and df_s[c].iloc[i+1])\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     x_split \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([df_s[c]\u001b[39m.\u001b[39miloc[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "import utils as utl\n",
    "from decision_tree import Decision_Tree_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Classifier:\n",
    "    def __init__(self) -> None:\n",
    "        self.res_dict = {}\n",
    "    \n",
    "    def fit(self, X, y, X_cat_cols):\n",
    "        self.res_dict = self.des_tree_nodes(X, y, X_cat_cols)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X.apply(lambda x: self.predict_single(x, self.res_dict), axis=1)\n",
    "\n",
    "    def predict_single(self, x, res_dict):\n",
    "        tp = res_dict['type']\n",
    "        attr = res_dict['attr']\n",
    "        if tp == 'cat':\n",
    "            res = res_dict['res'][x[attr]]\n",
    "        else:\n",
    "            val = res_dict['vals']\n",
    "            i = self.recover_index(x[attr],val)\n",
    "            res = res_dict['res'][str(i)]\n",
    "        \n",
    "        if isinstance(res,dict):\n",
    "            return self.predict_single(x, res)\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def new_nodes(self, X: pd.DataFrame, y: pd.Series, attr_max_gain: str, X_cat_cols) -> Dict:\n",
    "        dct_res = {}\n",
    "        cols = list(X.columns)\n",
    "        cols.remove(attr_max_gain)\n",
    "        X_upd = X[cols]\n",
    "        Xcc = X_cat_cols.copy()\n",
    "        if attr_max_gain in X_cat_cols:\n",
    "            Xcc.remove(attr_max_gain)\n",
    "        \n",
    "        dct_res['attr'] = attr_max_gain\n",
    "        if attr_max_gain in X_cat_cols:\n",
    "            dct_res['type'] = 'cat'\n",
    "            dct_res['vals'] = np.sort(X[attr_max_gain].unique())\n",
    "            childs = {}\n",
    "\n",
    "            for u in dct_res['vals']:\n",
    "                Xn = X_upd.loc[X[attr_max_gain]==u]\n",
    "                yn = y.loc[X[attr_max_gain]==u]\n",
    "                if len(yn)>0:\n",
    "                    childs[u] = yn.iloc[0]\n",
    "                    if len(yn.unique()) > 1:\n",
    "                        childs[u] = self.des_tree_nodes(Xn, yn, Xcc)\n",
    "            dct_res['res'] = childs\n",
    "        else:\n",
    "            dct_res['type'] = 'cont'\n",
    "            splits = self.get_splits(X[attr_max_gain], y)\n",
    "            dct_res['vals'] = splits\n",
    "            childs = {}\n",
    "            for i in range(len(splits) + 1):                \n",
    "                Xn = self.iloc_ranges(X_upd, X[attr_max_gain], splits, i)\n",
    "                yn = self.iloc_ranges(y, X[attr_max_gain], splits, i)\n",
    "                if len(yn)>0:\n",
    "                    childs[str(i)] = self.des_tree_nodes(Xn, yn, Xcc)\n",
    "            dct_res['res'] = childs\n",
    "        return dct_res\n",
    "    \n",
    "    def des_tree_nodes(self, X: pd.DataFrame, y: pd.Series, X_cat_cols):\n",
    "        cols = list(X.columns)\n",
    "        attr_max_gain = self.get_max_ig(X, y, X_cat_cols)\n",
    "        cols.remove(attr_max_gain)\n",
    "        if len(cols)!=0:\n",
    "            out = self.new_nodes(X, y, attr_max_gain, X_cat_cols)\n",
    "            return out\n",
    "        y_count = {}\n",
    "        for u in y.unique():\n",
    "            y_count[u] = y.loc[y==u].count()\n",
    "        out = max(y_count, key=y_count.get)\n",
    "        return out\n",
    "    \n",
    "    def info_gain_cat(self, x:pd.Series, y:pd.Series) -> float:\n",
    "        \"\"\"Calculates the info gain of using a given categorical attribute to describe categorical data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pd.Series\n",
    "            Categorical values of the attribute\n",
    "        y : pd.Series\n",
    "            Categorical target\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Information gain\n",
    "        \"\"\"\n",
    "        ES = self.entropy(y)\n",
    "        ESv = 0\n",
    "        nt = len(y)\n",
    "        if nt != 0:\n",
    "            for u in np.sort(x.unique()):\n",
    "                Sv = y.loc[x==u]\n",
    "                n = len(Sv)        \n",
    "                ESv += - (n/nt)*self.entropy(Sv)\n",
    "        return ES - ESv\n",
    "\n",
    "    def info_gain_con(self, x:pd.Series, y:pd.Series) -> float:\n",
    "        \"\"\"Calculates the info gain of using a given continuous attribute to describe categorical data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pd.Series\n",
    "            Categorical values of the attribute\n",
    "        y : pd.Series\n",
    "            Categorical target\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Information gain\n",
    "        \"\"\"\n",
    "        splits = self.get_splits(x, y)\n",
    "        ES = self.entropy(y)\n",
    "        ESv = 0\n",
    "        nt = len(y)\n",
    "        if nt != 0:\n",
    "            for i in range(len(splits) + 1):\n",
    "                Sv = self.iloc_ranges(y, x, splits, i)\n",
    "                n = len(Sv)\n",
    "                ESv += - (n/nt)*self.entropy(Sv)\n",
    "        return ES - ESv\n",
    "\n",
    "    def get_max_ig(self, X: pd.DataFrame, y: pd.Series, X_cat_cols: List) -> str:\n",
    "        \"\"\"Identifies the column of X with the maximum info gain related to the target y\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input data\n",
    "        y : pd.Series\n",
    "            Target data\n",
    "        X_cat_cols : List\n",
    "            List of categorical columns of X\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Name of the column with maximum information gain\n",
    "        \"\"\"\n",
    "        dct = {}\n",
    "        cols = list(X.columns)\n",
    "        for attr in cols:\n",
    "            if attr in X_cat_cols:\n",
    "                dct[attr] = self.info_gain_cat(X[attr], y)\n",
    "            else:\n",
    "                dct[attr] = self.info_gain_con(X[attr], y)\n",
    "        return max(dct, key=dct.get)\n",
    "    \n",
    "    def iloc_ranges(self, y: pd.Series, x: pd.Series, splits: List, i: int) -> pd.Series:\n",
    "        \"\"\"Filters the data from the series y for which the continuous data x is between the values split[i-1] and split[i]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pd.Series\n",
    "            Continuous data to use as base for the filtering\n",
    "        y : pd.Series or pd.DataFrame\n",
    "            Data to be filtered\n",
    "        splits : List\n",
    "            Values at which the values of x can be splitted\n",
    "        i : int\n",
    "            Index of the split to be used\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            Filtered data\n",
    "        \"\"\"\n",
    "        if len(x) > 0:\n",
    "            if i > 0 and i < len(splits):\n",
    "                Sv = y.loc[(x>splits[i-1]) & (x<=splits[i])]\n",
    "            elif i == 0:\n",
    "                Sv = y.loc[x<=splits[0]]\n",
    "            else:\n",
    "                Sv = y.loc[x>splits[-1]]\n",
    "            return Sv\n",
    "        return pd.Series([])\n",
    "\n",
    "    def recover_index(self, val: float, splits: List) -> int:\n",
    "        \"\"\"Recovers the index at which val falls whitin the range of the values specified by splits\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        val : pd.Series\n",
    "            Value to evaluate\n",
    "        splits : List\n",
    "            Limits of the split ranges.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Recovered index\n",
    "        \"\"\"\n",
    "        for i,v in enumerate(splits):\n",
    "            if i > 0:\n",
    "                if val > splits[i-1] and val <= v:\n",
    "                    return i\n",
    "            else:\n",
    "                if val <= v:\n",
    "                    return i\n",
    "        return len(splits)\n",
    "    \n",
    "    def get_splits(self, x:pd.Series, y:pd.Series) -> np.array:\n",
    "        \"\"\"Returns the splits of the categorical data x, using the median x for each category of y\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pd.Series\n",
    "            Input data for which the splits will be calculated\n",
    "        y : pd.Series\n",
    "            Target categorical data \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Limits of the splits\n",
    "        \"\"\"\n",
    "        vals = []\n",
    "        for u in y.unique():\n",
    "            x_np = x.loc[y==u].to_numpy()\n",
    "            vals.append(np.median(x_np))\n",
    "        vals_sorted = np.sort(np.unique(np.array(vals)))\n",
    "        return vals_sorted\n",
    "    \n",
    "    def entropy(self, y: pd.Series) -> float:\n",
    "        \"\"\"Calculates the entropy of y\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pd.Series\n",
    "            Categorical value for which the entropy will be calculated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Entropy\n",
    "        \"\"\"\n",
    "        entropy = 0\n",
    "        ntt = len(y)\n",
    "        if ntt != 0:\n",
    "            for u in y.unique():\n",
    "                y_c = y.loc[y==u]\n",
    "                p = len(y_c)/ntt\n",
    "                if p != 0:\n",
    "                    entropy += -p*math.log2(p)\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/1.raw/customerClassification.csv')#, parse_dates=['DateTime'],index_col=['DateTime'])\n",
    "df.columns\n",
    "X_cols = ['Gender', 'Ever_Married', 'Age', 'Graduated', 'Profession',\n",
    "       'Work_Experience', 'Spending_Score', 'Family_Size',\n",
    "       'Segmentation']\n",
    "X_cat_cols = ['Gender', 'Ever_Married', 'Graduated', 'Profession',\n",
    "       'Spending_Score', 'Segmentation']\n",
    "y_col = 'Var_1'\n",
    "\n",
    "X = df[X_cols]\n",
    "X = X.fillna(0)\n",
    "\n",
    "for cat in X_cat_cols:\n",
    "    X[cat] = X[cat].astype('category').cat.codes\n",
    "# for c in X.columns:\n",
    "#     X[c] = utl.min_max_scaling(X[c])[0]\n",
    "\n",
    "y = df[y_col].fillna(0).astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = Decision_Tree_Classifier()\n",
    "dt.fit(X, y, X_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = dt.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7222359940505702,\n",
       " 0.2236842105263158,\n",
       " 0.8095238095238095,\n",
       " 0.3505154639175258)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float_kind':'{:4.0f}'.format})\n",
    "cm = utl.confusion_matrix(y, y_p)\n",
    "acc = utl.accuracy_classification(cm)\n",
    "pres = utl.presicion_classification(cm)\n",
    "rec = utl.recall_classification(cm)\n",
    "f1 = utl.fbeta_classification(cm)\n",
    "\n",
    "acc, pres, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  17,    0,    0,    1,    5,    1,   52,    0],\n",
       "       [   0,   35,    3,    5,    5,    0,   84,    1],\n",
       "       [   0,    2,   93,    7,   12,    2,  306,    0],\n",
       "       [   0,    4,   11,  204,   43,    2,  556,    2],\n",
       "       [   1,    4,    8,   21,  387,    3,  659,    6],\n",
       "       [   0,    1,    1,    2,    5,   26,   50,    0],\n",
       "       [   3,   14,   26,   45,  117,    7, 5016,   10],\n",
       "       [   0,    0,    1,    3,   10,    0,  140,   49]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0).fit(X,y)\n",
    "y_pred_skl_np = clf.predict(X)\n",
    "y_pred_skl = pd.Series(y_pred_skl_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9556271690629649, 0.9342105263157895, 0.922077922077922, 0.9281045751633986)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_skl = utl.confusion_matrix(y, y_pred_skl)\n",
    "acc_skl = utl.accuracy_classification(cm_skl)\n",
    "pres_skl = utl.presicion_classification(cm_skl)\n",
    "rec_skl = utl.recall_classification(cm_skl)\n",
    "f1_skl = utl.fbeta_classification(cm_skl)\n",
    "\n",
    "acc_skl, pres_skl, rec_skl, f1_skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  71,    0,    0,    0,    0,    0,    5,    0],\n",
       "       [   0,  124,    0,    0,    1,    0,    8,    0],\n",
       "       [   1,    1,  399,    0,    1,    0,   20,    0],\n",
       "       [   0,    2,    9,  770,    2,    0,   38,    1],\n",
       "       [   2,    2,   16,   14, 1025,    0,   29,    1],\n",
       "       [   0,    0,    0,    0,    1,   83,    1,    0],\n",
       "       [   3,    4,   36,   61,   66,    4, 5063,    1],\n",
       "       [   0,    0,    1,    3,    4,    0,   20,  175]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_skl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
